# ──────────────────────────────────────────────────────────────────────
# CI/CD — GitHub Actions
# ──────────────────────────────────────────────────────────────────────
# Triggers: push to main/develop, PRs
# Jobs:
#   lint     — ruff linting
#   test     — pytest unit + evaluation tests
#   build    — Docker image build verification
# ──────────────────────────────────────────────────────────────────────

name: CI Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

env:
  PYTHON_VERSION: "3.11"
  LLM_BACKEND: "none"
  EMBEDDING_BACKEND: "sentence-transformers"

jobs:
  # ── Lint ────────────────────────────────────────────────────────────
  lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install ruff
        run: pip install ruff

      - name: Lint with ruff
        run: ruff check . --output-format=github

  # ── Unit Tests ─────────────────────────────────────────────────────
  test:
    runs-on: ubuntu-latest
    needs: lint
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: pip

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov httpx

      - name: Download spaCy model
        run: python -m spacy download en_core_web_sm

      - name: Run unit tests
        run: |
          pytest tests/ -v --tb=short --cov=. --cov-report=xml \
            -x --timeout=120

      - name: Run golden dataset F1 evaluation
        run: |
          python -c "
          from evaluation.golden_dataset import GoldenDataset, F1Evaluator
          ds = GoldenDataset()
          evaluator = F1Evaluator()
          # Baseline: keyword classifier
          preds = []
          for item in ds.get_all():
              text_lower = item['text'].lower()
              if any(w in text_lower for w in ['trade', 'economic', 'gdp', 'investment']):
                  preds.append('Economic')
              elif any(w in text_lower for w in ['defense', 'military', 'security', 'naval']):
                  preds.append('Security')
              else:
                  preds.append('Cultural')
          labels = [item['label'] for item in ds.get_all()]
          result = evaluator.evaluate(labels, preds)
          print(f'Macro F1: {result[\"macro_f1\"]:.3f}')
          assert result['macro_f1'] > 0.3, f'F1 too low: {result[\"macro_f1\"]}'
          "

      - name: Run adversarial test validation
        run: |
          python -c "
          from evaluation.adversarial_tests import AdversarialTester
          tester = AdversarialTester()
          print(f'Loaded {len(tester.test_cases)} adversarial test cases')
          assert len(tester.test_cases) >= 15
          "

      - name: Upload coverage
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coverage-report
          path: coverage.xml

  # ── Docker Build Verification ──────────────────────────────────────
  build:
    runs-on: ubuntu-latest
    needs: test
    steps:
      - uses: actions/checkout@v4

      - name: Build API image
        run: docker build -f docker/Dockerfile.api -t barometer-api:ci .

      - name: Build Dashboard image
        run: docker build -f docker/Dockerfile.dashboard -t barometer-dashboard:ci .

      - name: Verify API image
        run: |
          docker run --rm -d --name api-test -p 8000:8000 \
            -e LLM_BACKEND=none \
            -e EMBEDDING_BACKEND=sentence-transformers \
            barometer-api:ci
          sleep 10
          curl -f http://localhost:8000/health || exit 1
          docker stop api-test
